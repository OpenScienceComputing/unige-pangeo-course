{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Rechunk the kerchunked GOES dataset\n",
    "* Rechunk a small piece (24 time steps) of kerchunked GOES data from Lucas Sterzinger kerchunk tutorial\n",
    "* Show that loading many small chunks for each dask task can significantly improve performance\n",
    "* Show that using a cluster in the region where the data is located (using Coiled) can also improve performance  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import xarray as xr\n",
    "from rechunker import rechunk\n",
    "import zarr\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "User specific OSN parameters: endpoint, credentials and bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are using the ESIP OSN bucket at:\n",
    "osn_endpoint_url = 'https://ncsa.osn.xsede.org'  \n",
    "\n",
    "# AWS Secrets for writing to the ESIP OSN \"s3://esip\" Bucket\n",
    "osn_keys = '/shared/pangeo/nebari-setup/osn_keys.env'\n",
    "\n",
    "# And writing to the \"s3://esip\" \n",
    "temp_url = '/esip/pangeo-unige/rsignell/goes/tmp.zarr'\n",
    "target_url = '/esip/pangeo-unige/rsignell/goes/rechunked.zarr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv(osn_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(\"https://ncsa.osn.xsede.org/esip/rsignell/testing/combined.json\",\n",
    "                    engine='kerchunk', chunks={},\n",
    "                    backend_kwargs=dict(storage_options=dict(remote_protocol='s3', \n",
    "                                                             remote_options=dict(anon=True))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['SST'].encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['SST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nt = 1 * 12\n",
    "ny = 226 * 4\n",
    "nx = 226 * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(\"https://ncsa.osn.xsede.org/esip/rsignell/testing/combined.json\",\n",
    "                    engine='kerchunk', chunks={'t':nt, 'x':nx, 'y':ny},\n",
    "                    backend_kwargs=dict(storage_options=dict(remote_protocol='s3', \n",
    "                                                             remote_options=dict(anon=True))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds[['SST', 'DQF']]\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['SST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.nbytes/1e9   # GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_write = fsspec.filesystem('s3', anon=False, skip_instance_cache=True, \n",
    "            client_kwargs={'endpoint_url': osn_endpoint_url})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Remove temp and target zarr stores if they exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "try:\n",
    "    fs_write.rm(temp_url, recursive=True)\n",
    "except:\n",
    "    pass\n",
    "try: \n",
    "    fs_write.rm(target_url, recursive=True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_store = fs_write.get_mapper(temp_url)\n",
    "target_store = fs_write.get_mapper(target_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Drop a bunch of coords we don't care about:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.reset_coords(['day_solar_zenith_angle',\n",
    "                 'night_solar_zenith_angle',\n",
    "                 'quantitative_local_zenith_angle',\n",
    "                 'retrieval_local_zenith_angle',\n",
    "                 'retrieval_solar_zenith_angle',\n",
    "                 'x_image',\n",
    "                 'y_image'], drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster_type = 'Coiled'\n",
    "#cluster_type = 'Gateway'\n",
    "cluster_type = 'Nebari'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cluster_type == 'Coiled':\n",
    "    # Convert OSN S3 Credentials in .env file to python environment variables\n",
    "    import coiled\n",
    "\n",
    "    env_vars = {\"AWS_ACCESS_KEY_ID\":os.environ['AWS_ACCESS_KEY_ID'],\n",
    "            \"AWS_SECRET_ACCESS_KEY\":os.environ['AWS_SECRET_ACCESS_KEY']}\n",
    "\n",
    "    cluster = coiled.Cluster(\n",
    "        region=\"us-east-1\",\n",
    "        worker_options={\"nthreads\": 2},\n",
    "        compute_purchase_option=\"spot_with_fallback\",\n",
    "        n_workers=24,\n",
    "        environ=env_vars,\n",
    "        software='rechunk',\n",
    "        workspace='pangeo'\n",
    "    )\n",
    "\n",
    "    client = cluster.get_client()\n",
    "    max_mem = '11GB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cluster_type =='Gateway':\n",
    "    from dask_gateway import Gateway\n",
    "    gateway = Gateway()  # instantiate Dask gateway \n",
    "    # Cluster options on Nebari \n",
    "    gateway.cluster_options()    # to see options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cluster_type =='Gateway':\n",
    "# Convert OSN S3 Credentials in .env file to python environment variables\n",
    "\n",
    "    env_vars = {\"AWS_ACCESS_KEY_ID\":os.environ['AWS_ACCESS_KEY_ID'],\n",
    "            \"AWS_SECRET_ACCESS_KEY\":os.environ['AWS_SECRET_ACCESS_KEY']}\n",
    "    options = gateway.cluster_options()\n",
    "    options.conda_environment='global/global-pangeo'  # comment out for Daskhub or Planetary Computer\n",
    "    options.profile = 'Small Worker'   #  Small workers have 8GB RAM \n",
    "    options.environment_vars = env_vars\n",
    "    \n",
    "    # Create a Dask Gateway cluster\n",
    "    cluster = gateway.new_cluster(options)\n",
    "    \n",
    "    # Get the Dask client for the Dask Gateway cluster\n",
    "    client = cluster.get_client()\n",
    "    \n",
    "    # Scale the cluster\n",
    "    #cluster.adapt(minimum=2, maximum=20)\n",
    "    cluster.scale(24)\n",
    "    max_mem = '6GB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cluster_type == 'Nebari':\n",
    "    import sys, os\n",
    "    sys.path.append(os.path.join(os.environ['HOME'],'shared','pangeo','nebari-setup','lib'))\n",
    "    import nebari_tools as nbt\n",
    "    \n",
    "    aws_profile = 'osn-esip'\n",
    "    aws_region = 'us-west-2'\n",
    "    endpoint_url = f's3.{aws_region}.amazonaws.com'\n",
    "    \n",
    "    nbt.set_credentials(profile=aws_profile, region=aws_region, endpoint_url=endpoint_url)\n",
    "    worker_max = 24\n",
    "    \n",
    "    client, cluster = nbt.start_dask_cluster(profile=aws_profile, worker_max=worker_max, \n",
    "                                          region=aws_region, use_existing_cluster=True,\n",
    "                                          adaptive_scaling=False, wait_for_cluster=True, \n",
    "                                          worker_profile='Normal Worker', \n",
    "                                          propagate_env=True)\n",
    "    max_mem = '6GB'  # 75% of 8GB/worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_plan =  {'DQF': {'t':nt, 'y':ny, 'x':nx},\n",
    "               'SST': {'t':nt, 'y':ny, 'x':nx}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# calculate plan\n",
    "array_plan = rechunk(source=ds, target_chunks=chunk_plan, max_mem=max_mem, \n",
    "                    target_store=target_store, temp_store=temp_store, executor='dask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# execute plan\n",
    "array_plan.execute(retries=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr.convenience.consolidate_metadata(target_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "Try opening resulting rechunked file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_rechunked = xr.open_dataset(target_store, engine='zarr', chunks={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_rechunked.SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global-global-rechunk",
   "language": "python",
   "name": "conda-env-global-global-rechunk-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
