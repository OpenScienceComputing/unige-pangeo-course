{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Analyzing the National Water Model with Xarray, Dask, and Coiled\n",
    "\n",
    "This notebook was adapted from [this Coiled notebook](https://docs.coiled.io/user_guide/usage/dask/xarray.html) which in turn was adapted from [this blog post](https://github.com/dcherian/dask-demo/blob/main/nwm-aws.ipynb) by Deepak Cherian, Kevin Sampson, and Matthew Rocklin._\n",
    "\n",
    "<iframe class=\"plausible-event-name=youtube-coiled-xarray\" width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/blxvfGt9av8?si=-F_kY5K3VK4UvuPc\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n",
    "\n",
    "## The National Water Model Dataset\n",
    "\n",
    "In this example, we'll perform a county-wise aggregation of output from the National Water Model (NWM) available on the [AWS Open Data Registry](https://registry.opendata.aws/nwm-archive/). You can read more on the NWM from the [Office of Water Prediction](https://water.noaa.gov/about/nwm).\n",
    "\n",
    "## Problem description\n",
    "\n",
    "Datasets with high spatio-temporal resolution can get large quickly, vastly exceeding the resources you may have on your laptop. Dask integrates with Xarray to support parallel computing and you can use Coiled to scale to the cloud.\n",
    "\n",
    "We'll calculate the mean depth to soil saturation for each US county:\n",
    "\n",
    "- Years: 2020\n",
    "- Temporal resolution: 3-hourly land surface output\n",
    "- Spatial resolution: 250 m grid\n",
    "- 6 TB\n",
    "\n",
    "This example relies on a few tools:\n",
    "- `dask` + `coiled` process the dataset in parallel in the cloud\n",
    "- `xarray` + `flox` to work with the multi-dimensional Zarr datset and aggregate to county-level means from the 250m grid.\n",
    "\n",
    "## Before you start\n",
    "\n",
    "You'll first need to install the necessary packages. For the purposes of this example, we'll do this in a new virtual environment, but you could also install them in whatever environment you're already using for your project.\n",
    "\n",
    "```\n",
    "conda create -n coiled-xarray -c conda-forge python=3.10 coiled dask xarray flox rioxarray zarr s3fs geopandas geoviews matplotlib\n",
    "conda activate coiled-xarray\n",
    "```\n",
    "  \n",
    "You also could use `pip` for everything, or any other package manager you prefer; `conda` isn't required.\n",
    "\n",
    "When you later create a Coiled cluster, your local `coiled-xarray` environment will be automatically replicated on your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Start a Coiled cluster\n",
    "\n",
    "To demonstrate calculation on a cloud-available dataset, we will use Coiled to set up a dask cluster in AWS `us-east-1`."
   ]
  },
  {
   "cell_type": "raw",
   "id": "2",
   "metadata": {
    "tags": []
   },
   "source": [
    "import coiled\n",
    "\n",
    "cluster = coiled.Cluster(\n",
    "    name=\"xarray-nwm\",\n",
    "    region=\"us-east-1\", # close to dataset, avoid egress charges\n",
    "    n_workers=10,\n",
    "    tags={\"project\": \"nwm\"},\n",
    "    scheduler_vm_types=\"r7g.xlarge\", # memory optimized AWS EC2 instances\n",
    "    worker_vm_types=\"r7g.2xlarge\"\n",
    ")\n",
    "\n",
    "client = cluster.get_client()\n",
    "\n",
    "cluster.adapt(minimum=10, maximum=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(os.environ['HOME'],'shared','users','nebari-setup','lib'))\n",
    "import nebari_tools as nbt\n",
    "\n",
    "aws_profile = 'osn-esip'\n",
    "aws_region = 'none'\n",
    "endpoint_url = f's3.{aws_region}.amazonaws.com'\n",
    "\n",
    "nbt.set_credentials(profile=aws_profile, region=aws_region, endpoint_url=endpoint_url)\n",
    "worker_max = 30\n",
    "\n",
    "client, cluster = nbt.start_dask_cluster(profile=aws_profile, worker_max=worker_max, \n",
    "                                      region=aws_region, use_existing_cluster=True,\n",
    "                                      adaptive_scaling=True, wait_for_cluster=True, \n",
    "                                      worker_profile='Small', \n",
    "                                      propagate_env=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import flox  # make sure its available\n",
    "import fsspec\n",
    "import numpy as np\n",
    "import rioxarray\n",
    "import xarray as xr\n",
    "\n",
    "xr.set_options( # display options for xarray objects\n",
    "    display_expand_attrs=False,\n",
    "    display_expand_coords=False,\n",
    "    display_expand_data=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Load NWM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_aws = fsspec.filesystem('s3', anon=True, \n",
    "          config_kwargs={'connect_timeout':5, \n",
    "                         'read_timeout':5,\n",
    "                         'retries':{'max_attempts': 10}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "url = 's3://noaa-nwm-retrospective-2-1-zarr-pds/rtout.zarr'\n",
    "ds = xr.open_dataset(fs_aws.get_mapper(url), engine='zarr', chunks={})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Each field in this dataset is big!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds.zwattablrt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "tags": []
   },
   "source": [
    "Subset to a single year subset for demo purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subset = ds.zwattablrt.sel(time=slice(\"2020-01-01\", \"2020-12-31\"))\n",
    "subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Load county raster for grouping\n",
    "\n",
    "Load a raster TIFF file identifying counties by unique integer with [rioxarray](https://corteva.github.io/rioxarray/html/rioxarray.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import rioxarray\n",
    "\n",
    "fs = fsspec.filesystem(\"s3\", anon=True, \n",
    "                       client_kwargs=dict(endpoint_url='https://ncsa.osn.xsede.org'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "counties = rioxarray.open_rasterio(\n",
    "    'https://ncsa.osn.xsede.org/esip/example_data/Counties_on_250m_grid_cog.tif', chunks='auto',\n",
    ").squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any small floating point error in coordinate locations\n",
    "_, counties_aligned = xr.align(subset, counties, join=\"override\")\n",
    "\n",
    "counties_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "counties.encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "We'll need the unique county IDs later, calculate that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "county_id = np.unique(counties_aligned.data).compute()\n",
    "county_id = county_id[county_id != 0]\n",
    "print(f\"There are {len(county_id)} counties!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### GroupBy with flox\n",
    "\n",
    "We could run the computation as:\n",
    "\n",
    "```python\n",
    "subset.groupby(counties_aligned).mean()\n",
    "```\n",
    "\n",
    "This would use flox in the background, however, it would also load `counties_aligned` into memory. To avoid egress charges, you can use `flox.xarray` which allows you to lazily groupby a Dask array (here `counties_aligned`) as long as you pass in the expected group labels in `expected_groups`. See the [flox documentation](https://flox.readthedocs.io/en/latest/intro.html#with-dask)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flox.xarray\n",
    "\n",
    "county_mean = flox.xarray.xarray_reduce(\n",
    "    subset,\n",
    "    counties_aligned.rename(\"county\"),\n",
    "    func=\"mean\",\n",
    "    expected_groups=(county_id,),\n",
    ")\n",
    "\n",
    "county_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "county_mean.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# since our dataset is much smaller now, we no longer need cloud resources\n",
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read county shapefile, combo of state FIPS code and county FIPS code as multi-index\n",
    "import geopandas as gpd\n",
    "\n",
    "counties = gpd.read_file(\n",
    "    \"https://www2.census.gov/geo/tiger/GENZ2020/shp/cb_2020_us_county_20m.zip\"\n",
    ").to_crs(\"EPSG:3395\")\n",
    "counties[\"STATEFP\"] = counties.STATEFP.astype(int)\n",
    "counties[\"COUNTYFP\"] = counties.COUNTYFP.astype(int)\n",
    "continental = counties[~counties[\"STATEFP\"].isin([2, 15, 72])].set_index([\"STATEFP\", \"COUNTYFP\"]) # drop Alaska, Hawaii, Puerto Rico\n",
    "\n",
    "# Interpret `county` as combo of state FIPS code and county FIPS code. Set multi-index:\n",
    "yearly_mean = county_mean.mean(\"time\")\n",
    "yearly_mean.coords[\"STATEFP\"] = (yearly_mean.county // 1000).astype(int)\n",
    "yearly_mean.coords[\"COUNTYFP\"] = np.mod(yearly_mean.county, 1000).astype(int)\n",
    "yearly_mean = yearly_mean.drop_vars(\"county\").set_index(county=[\"STATEFP\", \"COUNTYFP\"])\n",
    "\n",
    "# join\n",
    "continental[\"zwattablrt\"] = yearly_mean.to_dataframe()[\"zwattablrt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7.68, 4.32))\n",
    "\n",
    "ax.set_axis_off()\n",
    "\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"bottom\", size='5%', pad=0.1)\n",
    "\n",
    "cax.tick_params(labelsize=8)\n",
    "cax.set_title(\"Average depth (in meters) of the water table in 2020\", fontsize=8)\n",
    "\n",
    "continental.plot(\n",
    "    column=\"zwattablrt\",\n",
    "    cmap=\"BrBG_r\",\n",
    "    vmin=0,\n",
    "    vmax=2,\n",
    "    legend=True,\n",
    "    ax=ax,\n",
    "    cax=cax,\n",
    "    legend_kwds={\n",
    "        \"orientation\": \"horizontal\",\n",
    "        \"ticks\": [0,0.5,1,1.5,2],\n",
    "        }\n",
    ")\n",
    "\n",
    "plt.text(0, 1, \"6 TB processed, ~$1 in cloud costs\", transform=ax.transAxes, size=9)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global-global-pangeo",
   "language": "python",
   "name": "conda-env-global-global-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
